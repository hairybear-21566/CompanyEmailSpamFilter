{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EAdECfOXX_Gv"
   },
   "source": [
    "<h1>Introduction</h1>\n",
    "<p>In these lab notes we will introduce you to Naïve Bayes and your task is to implement a relatively simple spam filter.  We will use a small data set to train and test the model (available on Blackboard). </p> \n",
    "\n",
    "<b>Note:</b>  Use Jupyter Notebook or Google Colab (NOT Jupyter Lab).\n",
    "    \n",
    "<p>The messages within the datasets have already been classified as spam, or ham (not spam). </p> \n",
    "\n",
    "<p>The implementation of a Naïve Bayes Spam Filter is relatively straight forward using scikit-learn, however, this library hides the implementation details (and many solutions are available on the Internet).  Therefore, this task does not permit the use of the scikit-learn library or similar libraries that perform similar tasks.  We hope that this will help with your understanding of the implementation details.</p> \n",
    "<p>Bayes Theorem can give us the probability that a message is spam S for a given event E</p>\n",
    "\n",
    "<h1>\n",
    "$P\\left(S\\middle|\\ E\\right)=\\frac{P\\left(E\\middle|\\ S\\right)P\\left(S\\right)}{P\\left(E\\middle|\\ S\\right)P\\left(S\\right)+P\\left(E|\\lnot S\\right)P\\left(\\lnot S\\right)}$\n",
    "</h1>\n",
    "\n",
    "<p>Where:</p>\n",
    "<p>$P\\left(S\\middle|\\ E\\right)$, the probability that the message is spam given the event occurred.</p>\n",
    "<p>$P\\left(S\\right)$, the prior probability that a message is spam.</p>\n",
    "<p>$P\\left(\\lnot S\\right)$, the prior probability that a message is not spam.  </p>\n",
    "\n",
    "Note:  $P\\left(S\\right)$ and $P\\left(\\lnot S\\right)$ are prior values, or prior beliefs.  This value could be calculated using the number of spam and number of ham classifications in the data set.  You could also use arbitrary values, for example; you could assume that of all email messages sent, 80% of them are spam and 20% of them are not spam.  The success of the filter depends on the prior values.\n",
    "\n",
    "<p>$P\\left(E\\middle|\\ S\\right)$, the probability that event E occurs in a spam emails.</p>\n",
    "\n",
    "<p>$P\\left(E|\\lnot S\\right)$, the probability that event E occurs in non-spam emails.</p>\n",
    "\n",
    "<h3>Additional Libraries</h3>\n",
    "<p>As you progress through the exercises you may need additional libraries that are not installed, for example when visualising the data you may need to import WordCloud.  If you use [Google Colaboratory](https://colab.research.google.com/notebooks/intro.ipynb) the necessary libraries will be available, which will save you having to pip install.  </p>\n",
    "<h3>The Implementation</h3>\n",
    "<p>We have tried to keep the implementation quite simple and therefore not accounted for things like filtering small often used words, nor does the implementation calculate probabilities of a word in the frequency list not appearing in spam.  We have also filtered things like telephone numbers, which could be used to identify spam.   The final exercise allows you to make recommendations how the model could be improved.    </p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ep6k47M5X_Gw"
   },
   "source": [
    "<h3>1.  Read the dataset into a dataframe and explore</h3>\n",
    "<p>Start by importing pandas and read the dataset into a DataFrame named df.  Output the first 20 rows of the dataframe to get a general feel of how the data is structured.</p>\n",
    "<p>You may encounter the error: UnicodeDecodeError: 'utf-8' codec can't decode bytes in position 135-136: invalid continuation byte.  You don't need to edit the datafile,  as you should be able to successfully read in the datafile by changing the encoding to latin-1.</p>\n",
    "<p>[1 Mark]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vq43eDWcX_Gx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1HwlygYX_G2"
   },
   "source": [
    "<h3>2. Clean the data</h3>\n",
    "<p>We are only interested in words, clean the data so that all punctuations are removed.  You should be left with a dataset that only contains alpha characters (including spaces).  You should also ensure all the words are lowercase.  Store the cleaned data into a DataFrame named clean.</p>\n",
    "\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Category</th>\n",
    "      <th>Message</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>ham</td>\n",
    "      <td>go until jurong point crazy available only in ...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>ham</td>\n",
    "      <td>ok lar joking wif u oni</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>spam</td>\n",
    "      <td>free entry in  a wkly comp to win fa cup final...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>ham</td>\n",
    "      <td>u dun say so early hor u c already then say</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>ham</td>\n",
    "      <td>nah i dont think he goes to usf he lives aroun...</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "[1 Marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqppPCO_X_G3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CLTcpAMvX_G8"
   },
   "source": [
    "<h3>3. Split the Data</h3>\n",
    "<p>Split the data into three random samples, one for training the model, one for validation and the other for testing the model.  Create DataFrames named train_data, validation_data and test_data.  The train_data DataFrame should contain 60-70% of the data, validation_data 15-20% and the test_data DataFrame the remaining data.<p>  \n",
    "\n",
    "<p>[1 Mark]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fgKwaz1yX_G9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EU8azvVgX_HB"
   },
   "source": [
    "<h3>4. Create a Word Frequency DataFrame</h3>\n",
    "<p>Create a new DataFrame named word_freq that contains each word with the number of times it appears in a spam and a ham message.  You should use the train_data.</p>\n",
    "<p>Below is an example of what the DataFrame would look like, <i>note</i> that your values may differ depending on how the data was split.</p>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Word</th>\n",
    "      <th>#Spam</th>\n",
    "      <th>#Ham</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>0</td>\n",
    "      <td>go</td>\n",
    "      <td>27</td>\n",
    "      <td>196</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>until</td>\n",
    "      <td>4</td>\n",
    "      <td>17</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2</td>\n",
    "      <td>jurong</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>3</td>\n",
    "      <td>point</td>\n",
    "      <td>1</td>\n",
    "      <td>9</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>4</td>\n",
    "      <td>crazy</td>\n",
    "      <td>4</td>\n",
    "      <td>8</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "      <td>...</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7253</td>\n",
    "      <td>salesman</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7254</td>\n",
    "      <td>pity</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7255</td>\n",
    "      <td>soany</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7256</td>\n",
    "      <td>suggestions</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7257</td>\n",
    "      <td>bitching</td>\n",
    "      <td>1</td>\n",
    "      <td>0</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p>[2 Marks]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJxL5N2BX_HC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1DBTOQpJbSqE"
   },
   "source": [
    "<h3>5. Visualise the Data</h3>\n",
    "<p>Let's use a Word Cloud library to visualise the most common words contained in spam messages.</p>\n",
    "\n",
    "[Example of a Word Cloud Image](https://drive.google.com/open?id=1lVRGHtMB1AMJf-JSi7MmcHbZB_BvBhGC)\n",
    "\n",
    "<p>[1 Marks]</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2mmdY8jWbkgS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVpFmJ5kX_HG"
   },
   "source": [
    "<h3>6.  Calculate $P\\left(E\\middle| S\\right)$ and $P\\left(E|\\lnot S\\right)$</h3>\n",
    "<p>Next create a new DataFrame named word_prob that gives the probability of each word being found in a spam and ham message.</p>\n",
    "<p>To calculate the probability of a word being spam you divide the number of times the word was found in spam by the total number of spam messages, likewise to calculate the probability of each word being found in a ham message you divide the number of times the word was found in a ham message by the total number of ham messages.</p>\n",
    "<p>If a word was not found in ham or spam it will cause problems later because the probability calculated will be zero. Therefore, use a pseudocount k and estimate the probability of seeing the word. This is known as smoothing and results in the following formula when k = 0.5, for example.</p>\n",
    "<p>$P\\left(E\\middle| S\\right)$ = (number of spams containing the word + k) / (total number of spam messages + 2 * k).</p>\n",
    "<p>Likewise, for $P\\left(E|\\lnot S\\right)$.</p>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Word</th>\n",
    "      <th>P(E|S)</th>\n",
    "      <th>P(E|¬S)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>go</td>\n",
    "      <td>0.053322</td>\n",
    "      <td>0.050055</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>until</td>\n",
    "      <td>0.011364</td>\n",
    "      <td>0.004275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>jurong</td>\n",
    "      <td>0.002622</td>\n",
    "      <td>0.000138</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>point</td>\n",
    "      <td>0.002622</td>\n",
    "      <td>0.002344</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>crazy</td>\n",
    "      <td>0.011364</td>\n",
    "      <td>0.002344</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</p>[2 Marks]</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2kaXmftX_HH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iagF4brWA5QO"
   },
   "source": [
    "<h3>7. Checking the 'spamliness' of a single word</h3>\n",
    "<p>Now that we have trained the model, we will test the model.  Before we use the test_data, first let’s check how the model calculates the spamliness of a single word.  This is where we use the Bayes Theorem formula.  We have already calculated $P\\left(E\\middle| S\\right)$ and $P\\left(E|\\lnot S\\right)$, so we can just extract these values from the word_prob DataFrame.</p>\n",
    "<p>We need to decide on the prior values $P\\left(S\\right)$ and $P\\left(\\lnot S\\right)$, this is where you can experiment and tweak the model, in this example the prior value for spam was set to $0.4$ and the prior value for not spam or ham was set to $0.6$.</p>\n",
    "<h3>\n",
    "$P\\left(S\\middle|\\ E\\right)=\\frac{P\\left(E\\middle|\\ S\\right)P\\left(S\\right)}{P\\left(E\\middle|\\ S\\right)P\\left(S\\right)+P\\left(E|\\lnot S\\right)P\\left(\\lnot S\\right)}$\n",
    "</h3>\n",
    "<pre>\n",
    "Output\n",
    "Word = ['free']\n",
    "P(E|S) = [0.29108392]\n",
    "P(E|¬S) = [0.01365141]\n",
    "P(S|E) = [0.93427577]\n",
    "P(¬S|E) = [0.06572423]\n",
    "</pre>\n",
    "\n",
    "<p>[2 Marks]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NS-KBvz4-YhJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NiU4nDFrNW-7"
   },
   "source": [
    "<h3>8. Checking the 'spamliness' of several words</h3>\n",
    "<p>To check the spamliness of several words contained in a message we multiply the probabilities.  The model assumes the words appear as independent events hence the naïve Bayes.  In reality of course, words are not independent events, but the model still performs well.  So we use the assumption that the words appear independently, and hence we multiply probabilities, so\n",
    "$P(S\\,|\\, x_1,\\dots,x_n)\\approx \\frac{P(S)\\underset{i=1}{\\overset{n}{\\prod}}P(x_i | S)}{P(S)\\underset{i=1}{\\overset{n}{\\prod}}P(x_i | S)+P(\\neg S)\\underset{i=1}{\\overset{n}{\\prod}}P(x_i | \\neg S)}$\n",
    "\n",
    "Calculate the probability for each word in a message being spam, you might want to store the calculations in a list named prob_spam.  Likewise create a list for each word not being spam.\n",
    "Then multiply the probabilities and compare the results.  If the result of multiplying the probabilities for spam is greater than the result of multiplying the probabilities for not spam, then you assume the message as spam.\n",
    "</p>\n",
    "<p>If you have a word in your message that is not in the word_prob DataFrame then you can't get the probability.  Skip any words in the message that are not in the word_prob DataFrame.</p>\n",
    "<p>[2 Marks]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGK027jUNlU7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oE6CsYaev-w_"
   },
   "source": [
    "<h3>9. Avoiding floating point underflow</h3>\n",
    "<p>Our aim is to compare two probabilities $P(S|x_1,\\dots,x_n)$ with $P(\\neg S|x_1,\\dots,x_n),$ according to our model introduced in Section 8, both probabilities share a common denominator which does not affect comparison. Hence we will calculate numerators only, which are proportional to $P(S|x_1,\\dots,x_n)$ and $P(\\neg S|x_1,\\dots,x_n).$\n",
    "</p>\n",
    "\n",
    "<p>Multiplying a set of small probabilities could result in a floating-point error.  This is where the product becomes too small to be represented correctly.  To avoid this we can take the logarithm of the probabilities and add them.  \n",
    "\n",
    "To avoid multiplication of small numbers, we use the following property of $\\log(x):$</p>\n",
    "$$\n",
    "\\log(a\\cdot b)=\\log(a)+\\log(b)\n",
    "$$\n",
    "<p>i.e. the log of the product is equal to the sum of logs (so instead of multiplying small numbers we will add them):</p>\n",
    "$$\n",
    "P(S|x_1,x_2,\\dots,x_n)\\propto P(S)\\cdot P(x_1|S)\\cdot \\dots \\cdot P(x_n|S)$$\n",
    "<p>becomes</p>\n",
    "$$\\log(P(S|x_1,x_2,\\dots,x_n))\\propto \\log\\left(P(S)\\cdot P(x_1|S)\\cdot \\dots  P(x_n|S)\\right)=$$ $$\n",
    "\\log(P(S))+\\log(P(x_1|S))+\\dots+\\log(P(x_n|S))\n",
    "$$\n",
    "<p>So, to check spam or ham we just compare:</p>\n",
    "$$\n",
    "\\log(P(S))+\\log(P(x_1|S))+\\dots+\\log(P(x_n|S))\n",
    "$$\n",
    "<p>and </p>\n",
    "$$\n",
    "\\log(P(\\neg S))+\\log(P(x_1|\\neg S))+\\dots+\\log(P(x_n|\\neg S))\n",
    "$$\n",
    "\n",
    "\n",
    "Change the equation so that logs are used.\n",
    "</p>\n",
    "<p>[2 Mark]</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4JYfU0vixQFw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtiGi5X37b0G"
   },
   "source": [
    "<h3>10. Testing the Model</h3>\n",
    "<p>Now that we have tested the model using simple messages.  Let’s test the model using the messages from the test_set.  You should implement counters that displays how your model has performed and calculate the accuracy of the model.</p>\n",
    "<pre>\n",
    "match_spam 173\n",
    "match_ham 843\n",
    "thought_ham_is_spam 3\n",
    "thought_spam_is_ham 357\n",
    "Accuracy: 0.7383720930232558\n",
    "</pre>\n",
    "<p>[3 Marks]</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOreoe6F7eri"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fwGqqNBzDh3y"
   },
   "source": [
    "<h3>11. Improvements</h3>\n",
    "<p>Utilise the validation set to assess the performance of various word sets in classifying spam and non-spam (ham) emails. Compare the effectiveness of different sets of words to determine their impact on classification accuracy.</p>\n",
    "<p>[3 Mark]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KM5hnY3WDA2f"
   },
   "source": [
    "<h3></h3>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lab05_spam_filter_student.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
